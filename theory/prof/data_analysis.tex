\chapter{Основы анализа данных}

\AddToShipoutPictureBG*{%
  \AtPageUpperLeft{%
    \hspace{\paperwidth}%
    \raisebox{-\baselineskip}{%
      \makebox[-10pt][r]{\textbf{ТИИ}}
}}}%

\begin{multicols}{2}
    \raggedcolumns
    \section{Основные понятия машинного обучения. Основные постановки задач. Примеры
    прикладных задач.}
    \subsection*{Основные постановки задач}
    Задача обучения по прецендентам (обучение с учителем);
    $X$ -- множество объектов, $Y$ -- множество ответов. Предполагается, что $\exists y: X \to Y$ -- зависимость между объектами и ответами (целевая функция, target function).
    \par
    \textbf{Постановка задачи: } Найти отображение \mbox{$a: \ X\to Y$} (алгоритм, решающая функция), приближающее $y$ на всем множестве $X$.
    \par 
    Отличия от классических задач аппроксимации -- более сложная структура пространств. 
    \bigskip\par
    Как могут задаваться объекты. Рассмотрим отображение \mbox{$f_j: \ X_j \to D_j$}, \mbox{$j = 1, \ldots, n$} -- признаки объектов (features).
    \par \textbf{Типы признаков}:
    \begin{itemize}
      \item[] $D_j = \{0, 1\}$ -- бинарный признак $f_j$;
      \item[] $|D_j| < \infty$ -- номинальный признак $f_j$;
      \item[] $|D_j| < \infty, \ D_j$ упорядочено -- порядковый признак $f_j$;
      \item[] $D_j = \R$ -- количественный признак $f_j$.
    \end{itemize}
    Вектором $[f_1(x), \ldots, f_n(x)]$ назовем признаковое описание объекта. Тогда можно составить матрицу $F$ ``объекты-признаки'' (feature data) следующего вида:
    \[
        F = \big|\big|f_j(x_i)\big|\big|_{l\times n} = \begin{bmatrix}
          f_1(x_1) & \ldots & f_n(x_1)\\
          \vdots & \ddots & \vdots\\
          f_1(x_l) & \ldots & f_n(x_l)
        \end{bmatrix}      
    \]
    \subsection*{Типы задач (по типу ответов)}:
    \textbf{Задача классификации}
    \begin{itemize}
      \item $Y = \{-1, +1\}$ -- классификация на 2 класса;
      \item $Y = \{1, \ldots, M\}$ -- на $M$ непересекающихся классов;
      \item $Y = \{0,1 \}^M$ -- на $M$ классов, которые могут пересекаться.
    \end{itemize}
    \textbf{Задача восстановлении регрессии}
    \begin{itemize}
      \item $Y = \R$ или $Y = \R^m$.
    \end{itemize}
    \textbf{Задача ранжирования}
    \begin{itemize}
      \item $Y$ -- конечное упорядоченное множество.
    \end{itemize}
    \textbf{Модель} (predictive model) -- параметрическое семейство функций:
    \[
        A = \{g(x, \theta) \ \big| \ \theta \in \Theta\},
    \]
    где $g: \ X \times \Theta \to Y$ -- фиксированная функция, $\Theta$ -- множество допустимых значений параметра $\theta$.
    \bigskip \par
    \subsection*{Метод обучения}
    \textbf{Этап обучения} Метод обучения(learning algorithm) $\mu: (X\times Y)^l \to A$: по выборке $X^l = (x_i, y_i)_{i=1}^l$ строится алгоритм $a = \mu(X^l)$:
    \[
        \begin{bmatrix}
          f_1(x_1) & \ldots & f_n(x_1)\\
          \vdots & \ddots & \vdots\\
          f_1(x_l) & \ldots & f_n(x_l)
        \end{bmatrix} \overset{y}{\to} \begin{bmatrix}
          y_1 \\
          \vdots\\
          y_l
        \end{bmatrix} \overset{\mu}{\to} a
    \] 
    \textbf{Этап применения}: алгоритм $a$ для новых объектов $x'_i$ выдает ответы $a(x_i')$:
    \[
        \begin{bmatrix}
          f_1(x'_1) & \ldots & f_n(x'_1)\\
          \vdots & \ddots & \vdots\\
          f_1(x_k) & \ldots & f_n(x'_k)
        \end{bmatrix} \overset{a}{\to} \begin{bmatrix}
          a(x'_1)\\
          \vdots\\
          a(x'_k)
        \end{bmatrix}
    \]
    \subsection*{Функционалы качества}
    $\Lf(a,x)$ -- функция потерь (loss function) -- величина ошибки алгоритм $a\in A$ на объекте $x\in X$.
    \par
    Для разных типов задач используют разные функционалы потерь.
    \par
    \textbf{Функции потерь для классификации}
    \begin{itemize}
      \item $\Lf(a,x) = \left[a(x) \neq y(x)\right]$ -- индикатор ошибки;
    \end{itemize}
    \par
    \textbf{Функции потерь для задач регрессии}
    \begin{itemize}
      \item $\Lf(a,x) = \left|a(x) - y(x)\right|$ -- абсолютное значение ошибки;
      \item $\Lf(a,x) = \left(a(x) - y(x)\right)^2$ -- квадратичная ошибка.
    \end{itemize}   
    Эмпирический риск -- функционал качества алгоритма $a$ на $X^l$:
    \[
        Q(a, X^l) = \dfrac{1}{l} \suml_{i=1}^{l} \Lf(a,x_i).
    \]
    \subsection*{Сведение задачи обучения к задаче оптимизации}
    Метод минимизации эмпирического риска (Empirical Risk Minimization):
    \[
        \mu(X^l) = \arg \min\limits_{a\in A} Q(a, X^l)
    \]
    Частный случай ERM -- МНК:
    \[
        \mu (X^l) = \arg\min\limits_{\theta} \suml_{i=1}^{l} \left(g(x_i, \theta) - y_i\right)^2
    \]
    \subsubsection*{Проблемы обучения}
    \begin{itemize}
      \item Недообучение (underfitting): модель слишком проста,недостаточное параметров;
      \item Переобучение (overfitting): модель слишком сложна, избыточное число параметров.
    \end{itemize}
    Минимизация переобучения:
    \begin{itemize}
      \item Минимизация HoldOut, LOO, CV;
      \item Регуляризация (ограничения на $\theta$).
    \end{itemize}
    Эмпирический риск на тестовых данных ($hold-out$):
    \[
        HO(\mu. X^l, X^k) = Q(\mu(X^l), X^k) \to \min
    \]
    \textbf{Недостаток: } зависимость от разбиения выборки.
    \par
    Скользящий контроль (leave-one-out), $L = l+1$:
    \[
        LOO(\mu, X^l) = \dfrac{1}{l}\suml_{i=1}^{L} \Lf (\mu(X^l \backslash\{x_i\}), x_{\mu}) \to \min
    \]
    \textbf{Недостаток: } обучение столько раз, сколько элементов в выборке (трудоемко вычислительно).
    \par
    Кросс-проверка (cross-validation), $L = l+k, X^L = X_n^l \cup X_n^k$:
    \[
        CV(\mu, X^L) = \dfrac{1}{|N|} \suml_{n\in N} Q\left(\mu (X^l_n), X^k_n\right) \to min
    \]
    \textbf{Сложность: } подбор $N$ (по факту, как разбивать)
    \subsection*{Примеры прикладных задач}
    \subsubsection*{Задача классификации}
    \par 
    \textbf{Задачи медицинской диагностики}. Объект -- пациент в определенный момент времени. Классы -- диагноз или способ лечения или исход заболевания.
    \par
    Примеры признаков:
    \begin{itemize}
      \item бинарные: пол, головная боль, и тд;
      \item порядковые: тяжесть заболевания, желтушность и тд;
      \item количественные: возраст, гормоны, давление и тд.
    \end{itemize}
    Особенности задачи:
    \begin{itemize}
      \item много пропусков в данных;
      \item интерпретируемый алгоритм;
      \item нужно выделять синдромы -- сочетания симптомов;
      \item нужна оценка вероятности отрицательного исхода.
    \end{itemize}
    \bigskip\par
    \textbf{Задача кредитного скоринга}. Объект -- заявка на выдачу банком кредита. Классы -- $\{0,1\}$.
    \par
    Примеры признаков:
    \begin{itemize}
      \item бинарные: пол, наличие судимости, и тд;
      \item номинальные: место проживания, профессия, и тд;
      \item порядковые: должность, образование, и тд;
      \item количественные: зарплата, возраст, стаж, и тд.
    \end{itemize}
    Особенность задачи: оценка вероятности дефолта $P(0)$.
    \bigskip\par
    \textbf{Задача предсказания оттока клиентов}. Объект -- абонент в определенный момент времени. Классы -- уйдет или не уйдет в следующем месяце.
    \par
    Примеры признаков:
    \begin{itemize}
      \item бинарные: корпоративный клиент, включение услуг, и тд;
      \item номинальные: тарифный план, регион проживания, и тд;
      \item количественные: длительность разговоров (входящих, исходящих, СМС и тд), и тд.
    \end{itemize}
    Особенности задачи:
    \begin{itemize}
      \item большие выборки;
      \item вероятность ухода;
      \item сырые данные.
    \end{itemize}
    \bigskip\par 
    \textbf{Задача категоризации текстовых документов}. Объект -- текстовый документ. Классы -- рубрики иерархического тематического каталога.
    \par
    Примеры признаков:
    \begin{itemize}
      \item номинальные: автор, издание, год и тд;
      \item количественные: частота в тексте, в заголовках и тд для каждого термина.
    \end{itemize}
    Особенности задачи:
    \begin{itemize}
      \item разметка данных;
      \item пересечение классов;
      \item в каждом ребре дерева классификатор на 2 класса.
    \end{itemize}
    \subsubsection*{Задача регрессии}
    \textbf{Задача прогнозирования стоимости недвижимости}. Объект -- недвижимость. 
    \par
    Примеры признаков: 
    \begin{itemize}
      \item бинарные: наличие балкона, лифта и тд;
      \item номинальные: район, тип дома, тип недвижимости, и тд;
      \item количественные: число комнат, площадь, количество санузлов, и тд.
    \end{itemize}
    Особенности задачи:
    \begin{itemize}
      \item выборка неоднородна, стоимость меняется со временем;
      \item разнотипные признаки;
      \item преобразования признаков.
    \end{itemize}
    \textbf{Задача прогнозирования объемов продаж}. Объект -- тройка (товар, магазин, день)
    \par
    Примеры признаков:
    \begin{itemize}
      \item бинарные: выходной день, праздник, и тд;
      \item количественные: объемы продаж, средний чек и тд.
    \end{itemize}
    Особенности задачи:
    \begin{itemize}
      \item функция потерь не квадратична и не симметрична;
      \item разреженные данные.
    \end{itemize}
    \subsection*{Задача ранжирования}
    \textbf{Задача ранжирования поисковой выдачи}. Объект -- пара (короткий текстовый запрос, документ). Классы -- релевантен или нет, разметка делается людьми -- асессорами.
    \par
    Примеры количественных признаков
    \begin{itemize}
      \item частота слов запроса в документе;
      \item число ссылок;
      \item число кликов на документ.
    \end{itemize}
    Особенности задачи:
    \begin{itemize}
      \item big data;
      \item оптимизируется не число ошибок, а качество ранжирования;
      \item проблема конструирования признаков по сырым данным.
    \end{itemize}
    \subsection*{Методология машинного обучения}
    \subsubsection*{Особенности данных}
    \begin{itemize}
      \item разнородные (разные шкалы);
      \item неполные (пропуски);
      \item неточные (погрешности);
      \item противоречивые (разные ответы на одних объектах);
      \item избыточные (сверхбольшие);
      \item недостаточные (объектов меньше, чем признаков);
      \item неструктурированные (нет признаковых описаний).
    \end{itemize}
    Риски, связанные с постановкой задачиЖ
    \begin{itemize}
      \item грязные данные;
      \item неясные критерии качества модели.
    \end{itemize}
    \section{Линейные пространства. Векторы и матрицы. Линейная независимость. Обратная
    матрица.}

    \section{Производная и градиент функции. Градиентный спуск. Выпуклые функции.}

    \section{Случайные величины. Дискретные и непрерывные распределения. Примеры.}

    \section{Оценивание параметров распределений, метод максимального правдоподобия.
    Бутстрэппинг.}

    \section{Линейные методы классификации и регрессии: функционалы качества, методы
    настройки, особенности применения.}
    \subsubsection*{Оптимизация регрессии}
    Обучающая выборка $X^l = \left(x_i, y_i\right)_{i=1}^l, \ x_i \in \R^n, \ y_i \in \R$
    \begin{enumerate}
      \item Модель регрессии -- линейная:
      \[
          a(x,w) = \langle x, w\rangle = \suml_{j=1}^n w_jf_j(x), \hspace*{0.5cm} w\in \R^n 
      \]
      \item Функция потерь, например, квадратичная:
      \[
          \Lf(a,y) = \left(a-y\right)^2
      \]
      \item Метод обучения -- метод наименьших квадратов:
      \[
          Q(w) = \suml_{i=1}^l\left(a(x_i, w) - y_i\right)^2 \to \min\limits_{w}
      \]
      \item Проверка по тестовой выборке $X^k = \left(\tilde{x}_i, \tilde{y}_i\right)_{i=1}^k$:
      \[
          \overline{Q}(w) = \dfrac{1}{k}\suml_{i=1}^k\left(a(\tilde{x}_i, w) - \tilde{y}_i\right)^2
      \]
    \end{enumerate} 
    \subsubsection*{Оптимизации классификации}
     Обучающая выборка: $X^l = \left(x_i, y_i\right)_{i=1}^l, \ x_i\in \R^n, \ y_i \in \{-1, +1\}$
     \begin{enumerate}
      \item Модель классификации -- линейная:
      \[
          a(x,w) = \sign \langle x,w \rangle = \sign \suml_{j=1}^n w_jf_j(x)
      \]
      \item Функция потерь -- бинарная или ее апроксимация:
      \[
          \Lf (a, y) = [ay < 0] = [\langle x,w \rangle < 0] \leq \Lf \left(\langle x, w \rangle y\right)
      \]  
      \item Метод обучения -- минимизация эмпирического риска:
      \[
          Q(w) = \suml_{i=1}^l \left[\langle x_i, w\rangle y_i < 0\right] \leq \suml_{i=1}^l \Lf \left(\langle x_i, w\rangle y_i\right) \to \min\limits_w
      \]
      \item Проверка по тестовой выборке $X^k = \left(\tilde{x}_i, \tilde{y}_i\right)_{i=1}^k$:
      \[
          \overline{Q}(w) = \dfrac{1}{k}\suml_{i=1}^k \left[\langle\tilde{x}_i, w\rangle \tilde{y}_i < 0\right]
      \]
     \end{enumerate}
     \subsection*{Метод градиентного спуска + стохастического градиентного спуска}
     Минимизация эмпирического риска:
     \[
        Q(w) = \suml_{i=1}^l \Lf_i(w) \to \min\limits_w
     \] 
     Численная минимизация методом ГС:
     \begin{itemize}
      \item[] $w^{(0)}$ -- начальное приближение
      \item[] Шаг $t+1$:
      \[
          w^{(t+1)} := w^{(t)} - h\cdot \grad Q(w^{(t)}), \hfill \grad{Q(w)} = \left(\dfrac{\partial Q(w)}{\partial w_j}\right)_{j=0}^n
      \]
      где $h$ -- градиентный шаг (темп обучения)
      \[
        w^{(t+1)} := w^{(t)} - h\cdot\suml_{i=1}^l \grad \Lf_i(w^{(t)}).
        \]
     \end{itemize}
     Идея ускорения сходимости: брать $(x_i, y_i)$ по одному и сразу обновлять вектор весов.
     \par
     \textbf{Вход: } выборка $X^l$, темп обучения $h$, темп забывания $\lambda$;
     \par
     \textbf{Выход: } вектор весов $w$;
      \begin{algorithmic}[1]
        \State инициализировать веса $w_j, \ j=0,\ldots, n$;
        \State инициализировать оценку функционала:
        \[
            \overline{Q} := \dfrac{1}{l}\suml_{i=1}^l \Lf_i(w)
        \]
        \Repeat
        \State выбрать объект $x_i \in X^l$ случайным образом;
        \State вычислить потерю: $\varepsilon_i = \Lf_i(w)$;
        \State сделать градиентный шаг: \mbox{$w = w - h\grad{\Lf}_i(w)$};
        \State оценить функционал: $\overline{Q} = \lambda \varepsilon_i + (1-\lambda)\overline{Q}$;
        \Until значение $\overline{Q}$ и/или веса $w$ не сойдутся.
      \end{algorithmic}
    $\lambda$ -- скользящее экспоненциальное среднее.
    \subsubsection*{Улучшения}
    Идея: делать градиент не на одном случайном объекте, а на группе.
    \par 
    Метод накопления импульса:
    \[
        \begin{array}{c}
          v = \gamma v + (1-\gamma)\Lf'_i(w)\\
          w = w - hv
        \end{array}
    \]
    NAG (Nesterov's accelerated gradient)
    \[
        \begin{array}{c}
          v = \gamma v+(1-\gamma)\Lf_i' (w-h\gamma w)\\
          w = w-hv
        \end{array}
    \]
    \section{Метрики качества алгоритм регрессии и классификации.}

    \section{Оценивание качества алгоритмов. Отложенная выборка, ее недостатки. Оценка полного
    скользящего контроля. Кросс-валидация. Leave-one-out.}
    Вопрос 6.
    \section{Деревья решений. Методы построения деревьев. Их регуляризация.}

    \section{Композиции алгоритмов. Разложение ошибки на смещение и разброс.}
    \columnbreak
    \section{Случайный лес, его особенности. Методы поиска выбросов в данных. Методы
    восстановления пропусков в данных. Работа с несбалансированными выборками.}

    \section{Нейронные сети: перцептрон, многослойный перцептрон. Автоэнкодеры и
    рекуррентные нейронные сети.}

    \section{Задача кластеризации. Алгоритм K-Means. Оценки качества кластеризации.
    Литература
    }
\end{multicols}